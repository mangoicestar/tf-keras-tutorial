{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "可參考 \n",
    "* https://en.wikipedia.org/wiki/Word2vec\n",
    "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "* https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# windows only hack for graphviz path \n",
    "import os\n",
    "for path in os.environ['PATH'].split(os.pathsep):\n",
    "    if path.endswith(\"Library\\\\bin\"):\n",
    "        os.environ['PATH']+=os.pathsep+os.path.join(path, 'graphviz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 設定環境變數來控制 keras, theano\n",
    "os.environ['KERAS_BACKEND']=\"tensorflow\"\n",
    "os.environ['THEANO_FLAGS']=\"floatX=float32, device=cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "with lzma.open(\"text8.xz\", \"rt\") as text8_file:\n",
    "    words = text8_file.read().split()\n",
    "\n",
    "print('Data size', len(words))\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先處理掉少用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總共有多少種字？\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們只考慮最常用的 50000 字， 其他字用 UNK 取代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "# 先統計字數\n",
    "counter = collections.Counter(words)\n",
    "# 可以看一下 counter 是的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最常見的 20 個字\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "wordfreq = counter.most_common(vocabulary_size-1)\n",
    "\n",
    "\n",
    "# 建立 編號: 字 的對照表\n",
    "num2word = ['UNK'] + [w for (w, _) in wordfreq]\n",
    "freq = np.array([0]+[n for (_, n) in wordfreq], dtype=\"float64\")\n",
    "freq[0] = len(words) - freq.sum()\n",
    "freq = (np.sqrt(freq/0.001)+1)*9.001/freq\n",
    "# 建立一個 字: 編號 的對照表\n",
    "word2num = {w: i for i, w in enumerate(num2word)}\n",
    "\n",
    "# 把 words 轉成對定的編號\n",
    "data = np.array([word2num.get(word, 0) for word in words])\n",
    "# 不需要 words 了\n",
    "del words\n",
    "del wordfreq\n",
    "freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下目前的狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:20])\n",
    "print(\" - \".join(num2word[n] for n in data[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成 skip-gram 模型的 batch\n",
    "\n",
    "keywords skipgram, cbow, n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Flatten, Input\n",
    "from keras.models import Sequential, Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# vector 的維度\n",
    "embedding_size = 128\n",
    "\n",
    "# 這其實只是線性映射，只不過輸入不是 one hot 而是 integer， 所以等同查表\n",
    "word2vec = Sequential()\n",
    "word2vec.add(Embedding(vocabulary_size, embedding_size, input_length=1))\n",
    "word2vec.add(Flatten())\n",
    "train_input = word2vec.inputs[0]\n",
    "embeddings = word2vec.layers[0].embeddings \n",
    "\n",
    "# 對應到的 context\n",
    "train_labels = Input(shape=(1,), dtype=\"int32\")\n",
    "\n",
    "# 這裡利用 tensorflow 的 nce_loss\n",
    "nce_W = K.variable(K.random_normal((vocabulary_size, embedding_size),stddev=(embedding_size)**-0.5))\n",
    "loss = K.mean(tf.nn.nce_loss(\n",
    "                     weights=nce_W,\n",
    "                     biases=K.zeros((vocabulary_size,)),\n",
    "                     labels=train_labels,\n",
    "                     inputs=word2vec.output,\n",
    "                     num_sampled=64, # Number of negative examples to sample.\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "# 利用 tensorflow 的 optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "# 之後要拿來檢驗的例子\n",
    "valid_examples = np.array([word2num[x] for x in [\"five\", \"many\", \"american\", \"time\", \"see\", \"war\", \"history\", \"he\"]])\n",
    "valid_size = len(valid_examples)\n",
    "valid_dataset = K.constant(valid_examples[:, None], \"int32\")\n",
    "valid_embeddings = word2vec(valid_dataset)\n",
    "\n",
    "# 正規化 embeddings, 來找 nearest neighbor \n",
    "normalized_embeddings = K.l2_normalize(embeddings, 1)\n",
    "similarity = K.dot(valid_embeddings, K.transpose(normalized_embeddings))\n",
    "\n",
    "# Add variable initializer.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_batch(data, batch_size, num_skips, skip_window):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window    \n",
    "    context_length = skip_window*2+1\n",
    "    X = np.ndarray(shape=batch_size, dtype=np.int32)\n",
    "    Y = np.ndarray(shape=batch_size, dtype=np.int32)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        for i in range(0, batch_size, num_skips):\n",
    "            X[i:i+num_skips] = data[idx+skip_window]            \n",
    "            context = data[idx:idx+context_length][np.arange(context_length) != skip_window]        \n",
    "            # subsampling 機率\n",
    "            #p = np.ones(2*skip_window)/2/skip_window\n",
    "            Y[i:i+num_skips] = np.random.choice(context, size=num_skips, replace=False)\n",
    "            idx = (idx+1)%(len(data)-context_length)\n",
    "        yield X[:, None], Y\n",
    "# 測試看看\n",
    "X,Y = next(skipgram_batch(data, 20, 4, 3))\n",
    "for x,y in zip(X, Y):\n",
    "    print(\"{} -> {}\".format(num2word[x[0]], num2word[y]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "batch_gen = skipgram_batch(data, batch_size=128, num_skips=4, skip_window=3)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    for step in range(0,200001):\n",
    "        X,Y = next(batch_gen)\n",
    "        feed_dict = {train_input: X, train_labels: Y[:, None]}\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        if step >0 and step %10000 == 0:\n",
    "            print(step, \"average loss\", average_loss/2000, time.time()-t0)\n",
    "            average_loss = 0\n",
    "        if step % 50000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = num2word[valid_examples[i]]\n",
    "                nearest = (-sim[i, :]).argsort()[1:8 + 1]\n",
    "                print(valid_word, [num2word[x] for x in nearest])\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim(v, num=10):\n",
    "    if isinstance(v, str):\n",
    "        v = w2v(v)\n",
    "    return [num2word[x] for x in  (final_embeddings @ v).argsort()[-num-1:-1][::-1]]\n",
    "def w2v(w):\n",
    "    return final_embeddings[word2num.get(w, 0)] \n",
    "find_sim('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sim('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sim('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sim(w2v('king')-w2v('men')+w2v(\"women\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用 t-sne 降低維度\n",
    "from sklearn.manifold import TSNE\n",
    "samples = 500\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:samples])\n",
    "labels = num2word[:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畫出來\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.scatter(low_dim_embs[:, 0], low_dim_embs[:, 1])\n",
    "for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i]\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=14,\n",
    "                 ha='right',\n",
    "                 va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
