{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 手寫數字辨識 (MNIST_RNN_TF)\n",
    "\n",
    "2017/07/08\n",
    "徐仕杰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips:\n",
    "- 記得要download data set: \n",
    "[Mnist](https://github.com/Backlu/tf-keras-tutorial/blob/master/basic/mnist.pkl.xz)\n",
    "- 在command前面加** ! **可以執行console command\n",
    "- 在command前面加** ? **可以查詢Help\n",
    "- 什麼是one-hot representation:\n",
    "[one-hot](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science)  \n",
    "<br>\n",
    "- import PIL error : pip install Pillow\n",
    "- import pandas error: pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "-  [Import Package & Functions](#import) \n",
    "-  [1. Import MNIST Data](#Import Data) \n",
    "-  [2. 開始Deep Learning - RNN](#開始Deep Learning)  \n",
    "  -  [A. 定義參數](#定義參數) \n",
    "  -  [B. 設計一個Model從X預測Y](#設計一個) \n",
    "  -  [C. 選一個loss function,](#選一個loss) \n",
    "  -  [D. 選一個optimizer](#選一個o) \n",
    "  -  [E. 開始執行訓練](#開始執行) \n",
    "  -  [F. 算一下正確率](#算一下正)  \n",
    "<br>\n",
    "-  [3. Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import'></a>\n",
    "## Import Package & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import lzma\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tfdot import tfdot\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showX(X, rows=1):\n",
    "    assert X.shape[0] % rows == 0\n",
    "    int_X = (X*255).clip(0,255).astype('uint8')\n",
    "    # N*784 -> N*28*28 -> 28*N*28 -> 28 * 28N\n",
    "    int_X_reshape = int_X.reshape(rows, -1,28,28).swapaxes(1,2).reshape(28*rows,-1)\n",
    "    display(Image.fromarray(int_X_reshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):  \n",
    "    with tf.name_scope('summaries_'+str(name)):  \n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean', mean)  \n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('stddev', stddev)  \n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  \n",
    "        tf.summary.histogram('histogram', var)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Import MNIST Data'></a>\n",
    "## 1. Import MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先把MNIST資料讀進來\n",
    "- Training Data: 訓練Model\n",
    "- Validataion Data: 訓練Model的時候, 同步監控目前模型的好壞\n",
    "- Testing Data: 訓練結束後, 評估模型的好壞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list裡的前面是picture X [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "後面是label Y [5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "with lzma.open(\"mnist.pkl.xz\", 'rb') as f:\n",
    "    train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print('list裡的前面是picture X',train_set[0])\n",
    "print('後面是label Y',train_set[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 50000\n",
      "validataion data size: 10000\n",
      "testing data size: 10000\n",
      "picture shape: (784,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = train_set\n",
    "validation_X, validation_y = validation_set\n",
    "test_X, test_y = test_set\n",
    "print('training data size:',len(train_X))\n",
    "print('validataion data size:',len(validation_X))\n",
    "print('testing data size:',len(test_X))\n",
    "print('picture shape:',train_X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把Y label變成one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y = np.eye(10)[train_y]\n",
    "test_Y = np.eye(10)[test_y]\n",
    "validation_Y = np.eye(10)[validation_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='開始Deep Learning'></a>\n",
    "## 2. 開始Deep Learning - RNN\n",
    "- What is RNN?![](img/rnn_2.png)\n",
    "- Deep Learning ABC \n",
    "    -  [A. 定義參數](#定義參數) \n",
    "    -  [B. 設計一個Model從X預測Y](#設計一個) \n",
    "    -  [C. 選一個loss function,](#選一個loss) \n",
    "    -  [D. 選一個optimizer](#選一個o) \n",
    "    -  [E. 開始執行訓練](#開始執行) \n",
    "    -  [F. 算一下正確率](#算一下正)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='定義參數'></a>\n",
    "### A. 定義參數(Placeholder, Variable, Constant)\n",
    "tips: 把要餵進Model的資料X,Y定義成placeholder, 把要讓電腦幫忙找的權重W,B定義成Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001 # learning rate\n",
    "n_inputs = 28 # 每一行的维度\n",
    "n_steps = 28 # 28 行\n",
    "n_hidden_unins = 128 # 中間RNN的hidden units\n",
    "n_classes = 10  # RNN最后的输出類別個数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_steps, n_inputs], name=\"X\")\n",
    "Y_ =tf.placeholder(tf.float32, [None, n_classes], name=\"Y_\")\n",
    "\n",
    "W = {\n",
    "    'in': tf.Variable(tf.random_uniform([n_inputs, n_hidden_unins], -1.0, 1.0), name=\"In_W\"), # 输入层到中间层权重矩阵\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_unins, n_classes], -1.0, 1.0), name=\"Out_W\"), # 中间层到输出层的权重\n",
    "}\n",
    "variable_summaries(W['in'],'In_W')\n",
    "variable_summaries(W['out'],'Out_W')\n",
    "B = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_unins]), name=\"In_Bias\"), # 偏置\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]), name=\"Out_Bias\"),\n",
    "}\n",
    "variable_summaries(B['in'],'In_Bias')\n",
    "variable_summaries(B['out'],'OUT_Bias')\n",
    "\n",
    "#tfdot()  太亂了不看~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='設計一個'></a>\n",
    "###  B. 設計一個Model從X & X history預測Y  \n",
    "$s(t)=f(Uw(t)+Ws(t-1)$  \n",
    "$y(t)=g(Vs(t))$  \n",
    "<br>\n",
    "$w(t)$ : the current input   \n",
    "$s(t-1)$ : the values in the hidden layer from the previous time step   \n",
    "$s(t)$ : the current values hidden in the hidden layer    \n",
    "$y$ : probability distribution of the picture   \n",
    "$W, U$ : the weight matrices between input and hidden layer   \n",
    "$V$ : the weight matrices between hidden and output layer   \n",
    "\n",
    "![](img/rnn_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1. Input Layer 輸入層\n",
    "- 第一步：: 本来数据的維度是(batch_size,28,28)但是這樣無法與權重Ｗ相乘, 因此需要轉為(-1,28), 這裡的-1表示任意長度, 就相當於把原來有batch_size這麼多張的圖片, 全部疊在一起\n",
    "- 第二部：h=wx+b\n",
    "- 第三步：後面RNN Cell那一層需要序列數據，因此又需要把數據變成(batch_size, 28, 128)的維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden_layer for input\n",
    "# X : (128, 28, 28)\n",
    "with tf.name_scope(\"inLayer\"):\n",
    "    X_ = tf.reshape(X, [-1, n_inputs])  # [batch_size*28, 28]\n",
    "    X_in = tf.matmul(X_, W['in']) + B['in'] # (-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_unins]) # (batch_size, 28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2. Hidden Layer 隱藏層\n",
    "- 第一步：定義一個LSTM Cell\n",
    "    - Base interface for all RNN Cells\n",
    "        - tf.contrib.rnn.RNNCell\n",
    "    - Core RNN Cells for use with TensorFlow's core RNN methods\n",
    "        - tf.contrib.rnn.BasicRNNCell\n",
    "        - tf.contrib.rnn.BasicLSTMCell\n",
    "        - tf.contrib.rnn.GRUCell\n",
    "        - tf.contrib.rnn.LSTMCell\n",
    "        - tf.contrib.rnn.LayerNormBasicLSTMCell\n",
    "- 第二步：把input輸入rnn cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_CELL\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_unins) \n",
    "    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(lstm_cell, X_in, dtype=tf.float32) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3. Output Layer 輸出層\n",
    "- states只存最後的結果, 因為是LSTM, 所以分別人了long,short兩條線的結果, 這邊直接取state[1]\n",
    "- outputs包含了每一步的輸出, 這邊我們需要的是最後一步, 所以需要先把outputs tensor解開(unpack)變成列表[(batch, outputs)..] * steps\n",
    "- 上述取其中一種方法就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('outlayer'):\n",
    "    pred = tf.matmul(rnn_states[1], W['out']) + B['out']\n",
    "    #rnn_outputs = tf.unstack(tf.transpose(rnn_outputs, [1,0,2]))  \n",
    "    #pred = tf.matmul(rnn_outputs[-1], weights['out']) + biases['out'] \n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='選一個loss'></a>\n",
    "###  C. 選一個loss function, 當作Machine learning的目標\n",
    "- cross_entorpy $-log(\\Pr(Y_{true}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='選一個o'></a>\n",
    "### D. 選一個optimizer, 根據Data和我們訂的目標找參數W, B\n",
    "- 就先用我們上次講的梯度下降Optimizer吧, 總之可以幫我們找到最小的cost function就好了~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "#optimizer = tf.train.AdagradOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='開始執行'></a>\n",
    "### E. 開始執行訓練(Training Data + Validataion Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "with tf.name_scope('performance'):  \n",
    "    loss_scar = tf.summary.scalar('loss', loss)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    acc_scar = tf.summary.scalar('accuracy', accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先創造一個session, 然後記得要init variable\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "if not os.path.exists('tb-log_5'):\n",
    "    os.mkdir('tb-log_5')\n",
    "summary_writer_train = tf.summary.FileWriter('tb-log1/train',graph=tf.get_default_graph())\n",
    "summary_writer_validation = tf.summary.FileWriter('tb-log1/validation',graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, batch:300 loss:0.1724073588848114, acc:0.9465000033378601\n",
      "epoch:1, batch:300 loss:0.10900198668241501, acc:0.9667000174522446\n",
      "epoch:2, batch:300 loss:0.0846836268901825, acc:0.97320002317428598\n",
      "epoch:3, batch:300 loss:0.0741218775510788, acc:0.97839999198913576\n",
      "epoch:4, batch:300 loss:0.06407832354307175, acc:0.9804999828338623\n",
      "epoch:5, batch:300 loss:0.06771568953990936, acc:0.9801999926567078\n",
      "epoch:6, batch:300 loss:0.08114008605480194, acc:0.9757999777793884\n",
      "epoch:7, batch:300 loss:0.06521962583065033, acc:0.9824000000953674\n",
      "epoch:8, batch:300 loss:0.06359043717384338, acc:0.9836999773979187\n",
      "epoch:9, batch:300 loss:0.06809202581644058, acc:0.9835000038146973\n",
      "epoch:10, batch:300 loss:0.05826671048998833, acc:0.98540002107620246\n",
      "epoch:11, batch:300 loss:0.05615396052598953, acc:0.9864000082015991\n",
      "epoch:12, batch:300 loss:0.06187184900045395, acc:0.98570001125335697\n",
      "epoch:13, batch:300 loss:0.0685330331325531, acc:0.98329997062683102\n",
      "epoch:14, batch:300 loss:0.057971417903900146, acc:0.9842000007629395\n",
      "epoch:15, batch:300 loss:0.06352801620960236, acc:0.9844999909400941\n"
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 128\n",
    "total_batch= len(train_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        rnd_idx = np.random.choice(train_X.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_X[rnd_idx]\n",
    "        batch_y = train_Y[rnd_idx]\n",
    "        batch_x = batch_x.reshape([batch_size, n_inputs, n_steps])\n",
    "        _, loss_s, acc_s, summary= sess.run([optimizer, loss_scar, acc_scar, merged_summary_op], feed_dict={X: batch_x, Y_:batch_y})\n",
    "        summary_writer_train.add_summary(summary, ep * total_batch + i)\n",
    "        if i%100 ==0:\n",
    "            batch_vx = validation_X.reshape([-1, n_inputs, n_steps])\n",
    "            loss_s, acc_s, summary= sess.run([loss_scar, acc_scar, merged_summary_op], feed_dict={X: batch_vx , Y_: validation_Y})\n",
    "            acc = accuracy.eval({X: batch_vx , Y_: validation_Y})\n",
    "            los=loss.eval({X: batch_vx , Y_: validation_Y})\n",
    "            summary_writer_validation.add_summary(summary, ep * total_batch + i)\n",
    "            updateProgress('epoch:{x0}, batch:{x4} loss:{x3}, acc:{x2}'.format(x0=ep,x1=i,x2=acc,x3=los,x4=i))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 抓出前五筆訓練資料的預測結果 Y_softmax (注意：這個是one-hot的格式)\n",
    "_ = train_X[21:30].reshape([-1, n_inputs, n_steps])\n",
    "pred_tmp = pred.eval(feed_dict={X: _ })\n",
    "pred_tmp_df = pd.DataFrame(pred_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [0 9 1 1 2 4 3 2 7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAAcCAAAAABghI05AAAGXElEQVR4nO1YbWxUVRp+oKWQFiio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\nj8Iv/+ddXsOvBT8AJ0+kL8nqYuIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=252x28 at 0x130D2AA58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:  [0 9 1 1 2 4 3 2 7]\n",
      "第21~30筆資料的比對結果:       0\n",
      "0  True\n",
      "1  True\n",
      "2  True\n",
      "3  True\n",
      "4  True\n",
      "5  True\n",
      "6  True\n",
      "7  True\n",
      "8  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_ = pred_tmp_df.apply(np.argmax,axis=1)\n",
    "print('predict: ',_.values)\n",
    "showX(train_X[21:30])\n",
    "print('answer: ',train_y[21:30])\n",
    "\n",
    "_ = train_X[21:30].reshape([-1, n_inputs, n_steps])\n",
    "_ = correct_pred.eval({X: _ , Y_: train_Y[21:30]})\n",
    "print('第21~30筆資料的比對結果:',pd.DataFrame(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='算一下正'></a>\n",
    "### F. 算一下正確率Accuracy (用Testing Data)\n",
    "- unseen data\n",
    "- generalize from the training set to the test set\n",
    "- 泛化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9843\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "_ = test_X.reshape([-1, n_inputs, n_steps])\n",
    "_ = accuracy.eval({X: _ , Y_: test_Y})\n",
    "print('Accuracy:',_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=tb-log1 --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reference'></a>\n",
    "## Reference\n",
    "- Statistical Language Models Based on Neural. Networks. Tomáš Mikolov. Ph. D. thesis, Brno University of Technology: http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf\n",
    "- Implementation Source Code: http://shomy.top/2017/01/02/tensorflow-rnn-example-3/\n",
    "- youtube教學：https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-08-RNN2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
